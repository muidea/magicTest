#!/usr/bin/env python3
"""
æ€§èƒ½ç›‘æ§å’ŒæŠ¥å‘Šå·¥å…·
ç›‘æ§æµ‹è¯•æ‰§è¡Œæ€§èƒ½ï¼Œç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""

import time
import psutil
import threading
import json
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import Dict, List, Optional
import statistics

@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: str
    test_name: str
    duration: float  # ç§’
    cpu_percent: float
    memory_mb: float
    api_calls: int = 0
    success_count: int = 0
    failure_count: int = 0
    error_messages: List[str] = None
    
    def __post_init__(self):
        if self.error_messages is None:
            self.error_messages = []

class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics: List[PerformanceMetrics] = []
        self.current_test = None
        self.start_time = None
        self.api_call_count = 0
        self.success_count = 0
        self.failure_count = 0
        self.error_messages = []
        self.monitoring_thread = None
        self.stop_monitoring = False
        
    def start_test(self, test_name: str):
        """å¼€å§‹æµ‹è¯•ç›‘æ§"""
        self.current_test = test_name
        self.start_time = time.time()
        self.api_call_count = 0
        self.success_count = 0
        self.failure_count = 0
        self.error_messages = []
        
        # å¯åŠ¨åå°ç›‘æ§çº¿ç¨‹
        self.stop_monitoring = False
        self.monitoring_thread = threading.Thread(target=self._monitor_resources)
        self.monitoring_thread.daemon = True
        self.monitoring_thread.start()
        
        print(f"ğŸ“Š å¼€å§‹ç›‘æ§æµ‹è¯•: {test_name}")
    
    def end_test(self):
        """ç»“æŸæµ‹è¯•ç›‘æ§"""
        if not self.current_test or not self.start_time:
            return
        
        duration = time.time() - self.start_time
        
        # åœæ­¢ç›‘æ§çº¿ç¨‹
        self.stop_monitoring = True
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=1)
        
        # è·å–æœ€ç»ˆèµ„æºä½¿ç”¨æƒ…å†µ
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory_info = psutil.Process().memory_info()
        memory_mb = memory_info.rss / 1024 / 1024  # è½¬æ¢ä¸ºMB
        
        # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡
        metrics = PerformanceMetrics(
            timestamp=datetime.now().isoformat(),
            test_name=self.current_test,
            duration=duration,
            cpu_percent=cpu_percent,
            memory_mb=memory_mb,
            api_calls=self.api_call_count,
            success_count=self.success_count,
            failure_count=self.failure_count,
            error_messages=self.error_messages.copy()
        )
        
        self.metrics.append(metrics)
        
        # æ‰“å°æµ‹è¯•æ‘˜è¦
        self._print_test_summary(metrics)
        
        # é‡ç½®çŠ¶æ€
        self.current_test = None
        self.start_time = None
    
    def record_api_call(self, success: bool = True):
        """è®°å½•APIè°ƒç”¨"""
        self.api_call_count += 1
        if success:
            self.success_count += 1
        else:
            self.failure_count += 1
    
    def record_error(self, error_message: str):
        """è®°å½•é”™è¯¯ä¿¡æ¯"""
        self.error_messages.append(error_message)
    
    def _monitor_resources(self):
        """åå°èµ„æºç›‘æ§çº¿ç¨‹"""
        while not self.stop_monitoring:
            # è¿™é‡Œå¯ä»¥è®°å½•æ›´è¯¦ç»†çš„èµ„æºä½¿ç”¨å†å²
            time.sleep(5)  # æ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
    
    def _print_test_summary(self, metrics: PerformanceMetrics):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print(f"\n{'='*60}")
        print(f"ğŸ“‹ æµ‹è¯•æ‘˜è¦: {metrics.test_name}")
        print(f"{'='*60}")
        print(f"æ‰§è¡Œæ—¶é—´: {metrics.duration:.2f}ç§’")
        print(f"CPUä½¿ç”¨ç‡: {metrics.cpu_percent:.1f}%")
        print(f"å†…å­˜ä½¿ç”¨: {metrics.memory_mb:.1f}MB")
        print(f"APIè°ƒç”¨æ¬¡æ•°: {metrics.api_calls}")
        print(f"æˆåŠŸ: {metrics.success_count} | å¤±è´¥: {metrics.failure_count}")
        
        if metrics.failure_count > 0:
            success_rate = metrics.success_count / metrics.api_calls * 100 if metrics.api_calls > 0 else 0
            print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        
        if metrics.error_messages:
            print(f"\né”™è¯¯ä¿¡æ¯ ({len(metrics.error_messages)}ä¸ª):")
            for i, error in enumerate(metrics.error_messages[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
                print(f"  {i}. {error}")
            if len(metrics.error_messages) > 5:
                print(f"  ... è¿˜æœ‰ {len(metrics.error_messages) - 5} ä¸ªé”™è¯¯")
    
    def generate_report(self, output_file: str = None):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        if not self.metrics:
            print("âš ï¸  æ²¡æœ‰æ€§èƒ½æ•°æ®å¯æŠ¥å‘Š")
            return
        
        print(f"\n{'='*60}")
        print("ğŸ“Š æ€§èƒ½åˆ†ææŠ¥å‘Š")
        print(f"{'='*60}")
        
        # æ€»ä½“ç»Ÿè®¡
        total_duration = sum(m.duration for m in self.metrics)
        avg_duration = statistics.mean([m.duration for m in self.metrics])
        total_api_calls = sum(m.api_calls for m in self.metrics)
        total_success = sum(m.success_count for m in self.metrics)
        total_failure = sum(m.failure_count for m in self.metrics)
        
        print(f"æ€»æµ‹è¯•æ•°: {len(self.metrics)}")
        print(f"æ€»æ‰§è¡Œæ—¶é—´: {total_duration:.2f}ç§’")
        print(f"å¹³å‡æµ‹è¯•æ—¶é—´: {avg_duration:.2f}ç§’")
        print(f"æ€»APIè°ƒç”¨: {total_api_calls}")
        print(f"æ€»æˆåŠŸ: {total_success} | æ€»å¤±è´¥: {total_failure}")
        
        if total_api_calls > 0:
            overall_success_rate = total_success / total_api_calls * 100
            print(f"æ€»ä½“æˆåŠŸç‡: {overall_success_rate:.1f}%")
        
        # æŒ‰æµ‹è¯•è¯¦ç»†ç»Ÿè®¡
        print(f"\n{'='*60}")
        print("ğŸ“ˆ è¯¦ç»†æµ‹è¯•æ€§èƒ½")
        print(f"{'='*60}")
        
        for metrics in self.metrics:
            print(f"\næµ‹è¯•: {metrics.test_name}")
            print(f"  æ—¶é—´: {metrics.duration:.2f}ç§’")
            print(f"  CPU: {metrics.cpu_percent:.1f}%")
            print(f"  å†…å­˜: {metrics.memory_mb:.1f}MB")
            print(f"  APIè°ƒç”¨: {metrics.api_calls}")
            
            if metrics.api_calls > 0:
                test_success_rate = metrics.success_count / metrics.api_calls * 100
                print(f"  æˆåŠŸç‡: {test_success_rate:.1f}%")
        
        # æ€§èƒ½å»ºè®®
        print(f"\n{'='*60}")
        print("ğŸ’¡ æ€§èƒ½å»ºè®®")
        print(f"{'='*60}")
        
        # æ‰¾å‡ºæœ€æ…¢çš„æµ‹è¯•
        slowest_test = max(self.metrics, key=lambda m: m.duration)
        fastest_test = min(self.metrics, key=lambda m: m.duration)
        
        if slowest_test.duration > 30:  # è¶…è¿‡30ç§’çš„æµ‹è¯•
            print(f"âš ï¸  '{slowest_test.test_name}' æµ‹è¯•è¾ƒæ…¢ ({slowest_test.duration:.2f}ç§’)")
            print("   å»ºè®®: æ£€æŸ¥æ˜¯å¦æœ‰ä¸å¿…è¦çš„ç­‰å¾…æˆ–ä¼˜åŒ–APIè°ƒç”¨")
        
        # æ£€æŸ¥é«˜å†…å­˜ä½¿ç”¨
        high_memory_tests = [m for m in self.metrics if m.memory_mb > 100]  # è¶…è¿‡100MB
        if high_memory_tests:
            print(f"âš ï¸  ä»¥ä¸‹æµ‹è¯•å†…å­˜ä½¿ç”¨è¾ƒé«˜:")
            for test in high_memory_tests:
                print(f"    - {test.test_name}: {test.memory_mb:.1f}MB")
            print("   å»ºè®®: æ£€æŸ¥å†…å­˜æ³„æ¼æˆ–ä¼˜åŒ–æ•°æ®åŠ è½½")
        
        # æ£€æŸ¥ä½æˆåŠŸç‡
        low_success_tests = []
        for m in self.metrics:
            if m.api_calls > 0:
                success_rate = m.success_count / m.api_calls * 100
                if success_rate < 90:  # æˆåŠŸç‡ä½äº90%
                    low_success_tests.append((m.test_name, success_rate))
        
        if low_success_tests:
            print(f"âš ï¸  ä»¥ä¸‹æµ‹è¯•æˆåŠŸç‡è¾ƒä½:")
            for test_name, rate in low_success_tests:
                print(f"    - {test_name}: {rate:.1f}%")
            print("   å»ºè®®: æ£€æŸ¥APIç¨³å®šæ€§æˆ–ç½‘ç»œè¿æ¥")
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        if output_file:
            report_data = {
                "generated_at": datetime.now().isoformat(),
                "summary": {
                    "total_tests": len(self.metrics),
                    "total_duration": total_duration,
                    "average_duration": avg_duration,
                    "total_api_calls": total_api_calls,
                    "total_success": total_success,
                    "total_failure": total_failure,
                    "overall_success_rate": overall_success_rate if total_api_calls > 0 else 0
                },
                "tests": [asdict(m) for m in self.metrics],
                "recommendations": {
                    "slow_tests": [slowest_test.test_name] if slowest_test.duration > 30 else [],
                    "high_memory_tests": [m.test_name for m in high_memory_tests],
                    "low_success_tests": [name for name, _ in low_success_tests]
                }
            }
            
            with open(output_file, 'w') as f:
                json.dump(report_data, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ“„ æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
    
    def clear(self):
        """æ¸…é™¤æ‰€æœ‰ç›‘æ§æ•°æ®"""
        self.metrics.clear()
        self.current_test = None
        self.start_time = None

# å…¨å±€ç›‘æ§å™¨å®ä¾‹
global_monitor = PerformanceMonitor()

def monitor_test(test_name):
    """è£…é¥°å™¨ï¼šç›‘æ§æµ‹è¯•å‡½æ•°æ€§èƒ½"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            global_monitor.start_test(test_name)
            try:
                result = func(*args, **kwargs)
                global_monitor.record_api_call(success=True)
                return result
            except Exception as e:
                global_monitor.record_api_call(success=False)
                global_monitor.record_error(str(e))
                raise
            finally:
                global_monitor.end_test()
        return wrapper
    return decorator

def record_api_call(success=True):
    """è®°å½•APIè°ƒç”¨"""
    global_monitor.record_api_call(success)

def record_error(error_message):
    """è®°å½•é”™è¯¯"""
    global_monitor.record_error(error_message)

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    monitor = PerformanceMonitor()
    
    # æ¨¡æ‹Ÿæµ‹è¯•1
    monitor.start_test("åŸºç¡€åŠŸèƒ½æµ‹è¯•")
    time.sleep(1)
    monitor.record_api_call(success=True)
    monitor.record_api_call(success=True)
    monitor.record_api_call(success=False)
    monitor.record_error("APIè°ƒç”¨è¶…æ—¶")
    monitor.end_test()
    
    # æ¨¡æ‹Ÿæµ‹è¯•2
    monitor.start_test("å¹¶å‘æµ‹è¯•")
    time.sleep(2)
    for i in range(5):
        monitor.record_api_call(success=True)
    monitor.end_test()
    
    # ç”ŸæˆæŠ¥å‘Š
    monitor.generate_report("performance_report.json")